---

- name: (configurate) | replace etc/hosts file
  template:
    src: hosts.j2
    dest: /etc/hosts


- name: (configurate) | set environments    
  lineinfile:   
    path: /etc/environment   
    line: "{{item}}"    
  with_items:    
    - "JAVA_HOME={{java_home}}"   
    - "HADOOP_HOME={{hadoop_home}}"


- name: (configurate) | add hadoop group
  group:
    name: hadoop
    state: present


- name: (configurate) | add hadoop user
  user:
    name: "{{hadoop_user}}"
    state: present
    groups: hadoop
    generate_ssh_key: yes


- name: (configurate) | fetch namenode public ssh keys
  become_user: "{{hadoop_user}}"
  shell: cat ~/.ssh/id_rsa.pub
  register: ssh_keys
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | set namenode ssh_key to local facts
  set_fact:
    namenode_ssh_key: "{{ssh_keys.stdout}}"
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | deploy keys on all slave servers
  authorized_key:
    user: "{{hadoop_user}}"
    key: "{{hostvars[hadoop_namenode]['namenode_ssh_key']}}"


- name: (configurate) | clear namenode known_hosts
  become_user: "{{hadoop_user}}"
  shell: echo "" > ~/.ssh/known_hosts
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | add localhost to namenode known_hosts
  become_user: "{{hadoop_user}}"
  shell: ssh-keyscan {{item}} >> ~/.ssh/known_hosts
  with_items:
    - 127.0.0.1
    - localhost
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | add slaves hostname to namenode known_hosts
  become_user: "{{hadoop_user}}"
  shell: ssh-keyscan {{hostvars[item]['ansible_hostname']}} >> ~/.ssh/known_hosts
  with_items: "{{hadoop_slaves}}"
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | add slaves ip to namenode known_hosts
  become_user: "{{hadoop_user}}"
  shell: ssh-keyscan {{item}} >> ~/.ssh/known_hosts
  with_items: "{{hadoop_slaves}}"
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | chown to hadoop user and group
  shell: chown -R {{hadoop_user}}:{{hadoop_group}} /usr/local/hadoop


- name: (configurate) | chmod to hadoop home
  shell: chmod -R 770 /usr/local/hadoop


- name: (configurate) | mkdir /var/run/hadoop
  file:
    path: "{{item}}"
    state: directory
    owner: "{{hadoop_user}}"
    group: "{{hadoop_group}}"
    mode: 0770
  with_items:
    - /etc/hadoop
    - /var/log/hadoop
    - /var/run/hadoop
    - /usr/local/hadoop/var/dfs/nn
    - /usr/local/hadoop/var/dfs/dn


- name: (configurate) | copy config directory to /etc/hadoop
  become_user: "{{hadoop_user}}"
  shell: cp -f /usr/local/hadoop/etc/hadoop/* /etc/hadoop/


- name: (configurate) | replace hadoop configurations
  template:
    src: "config/{{item}}.j2"
    dest: "/etc/hadoop/{{item}}"
    owner: "{{hadoop_user}}"
    group: "{{hadoop_group}}"
  with_items:
    - hadoop-env.sh
    - core-site.xml
    - hdfs-site.xml
    - mapred-env.sh
    - mapred-site.xml
    - yarn-env.sh
    - yarn-site.xml
    - slaves


- name: (configurate) | copy namenode services
  template:
    src: "systemd/{{item}}.j2"
    dest: "/lib/systemd/system/{{item}}"
  with_items:
    - hadoop-dfs-namenode.service
    - hadoop-yarn-resourcemanager.service
    - hadoop-yarn-proxyserver.service
    - hadoop-mr-jobhistory.service
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | copy secondarynamenode services
  template:
    src: "systemd/{{item}}.j2"
    dest: "/lib/systemd/system/{{item}}"
  with_items:
    - hadoop-dfs-secondarynamenode.service
  when: hadoop_secondarynamenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | copy datanode services
  template:
    src: "systemd/{{item}}.j2"
    dest: "/lib/systemd/system/{{item}}"
  with_items:
    - hadoop-dfs-datanode.service
    - hadoop-yarn-nodemanager.service


- name: (configurate) | systemd daemon-reload
  shell: systemctl daemon-reload
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | dfs namenode format
  become_user: "{{hadoop_user}}"
  shell: yes 'Y' | /usr/local/hadoop/bin/hdfs --config {{hadoop_conf_dir}} namenode -format
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | start namenode services
  service:
    name: "{{item}}"
    state: started
  with_items:
    - hadoop-dfs-namenode.service
    - hadoop-yarn-resourcemanager.service
    - hadoop-yarn-proxyserver.service
    - hadoop-mr-jobhistory.service
  when: hadoop_namenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | start secondarynamenode services
  service:
    name: "{{item}}"
    state: started
  with_items:
    - hadoop-dfs-secondarynamenode.service
  when: hadoop_secondarynamenode in hostvars[inventory_hostname]['ansible_all_ipv4_addresses']


- name: (configurate) | start datanode services
  service:
    name: "{{item}}"
    state: started
  with_items:
    - hadoop-dfs-datanode.service
    - hadoop-yarn-nodemanager.service


# ./hadoop-2.8.2/bin/hadoop jar hadoop-2.8.2/share/hadoop/tools/lib/hadoop-streaming-2.8.2.jar -file /home/vagrant/mapper.py -mapper /home/vagrant/mapper.py -file /home/vagrant/reducer.py -reducer /home/vagrant/reducer.py -input /hduser/willyhu/novels -output /hduser/willyhu/novels-output

# ./hadoop-2.8.2/bin/hdfs dfs -ls /hduser/willyhu/novels-output

# ./hadoop-2.8.2/bin/hdfs dfs -cat /hduser/willyhu/novels-output/part-00000
